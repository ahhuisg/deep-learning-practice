{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "monthly-active",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-provincial",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "historical-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset, test_dataset = list(train_iter), list(test_iter)\n",
    "\n",
    "counter = Counter()\n",
    "counter.update(['DUMMY'])\n",
    "for (label, line) in train_dataset:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informational-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 32\n",
    "num_timesteps = 50\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "rolled-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_pipeline = lambda x: [vocab[token] for token in x]\n",
    "label_pipeline = lambda x: int(x) - 1\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, idx_list = [], []\n",
    "    \n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        \n",
    "        tokens = tokenizer(_text)\n",
    "        if len(tokens) >= num_timesteps:\n",
    "            tokens = tokens[:num_timesteps]\n",
    "        else:\n",
    "            for _ in range(num_timesteps-len(tokens)):\n",
    "                tokens.append('DUMMY')\n",
    "\n",
    "        idx_list.append(embedding_pipeline(tokens))\n",
    "    \n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    idx_list = np.array(idx_list)\n",
    "\n",
    "    return label_list, idx_list\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-collectible",
   "metadata": {},
   "source": [
    "# Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hybrid-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim)\n",
    "\n",
    "        # Parameters for Forget Gate\n",
    "        self.f_x, self.f_h, self.f_b = self._get_params()\n",
    "\n",
    "        # Parameters for Output Gate\n",
    "        self.o_x, self.o_h, self.o_b = self._get_params()\n",
    "\n",
    "        # Parameters for Input Gate\n",
    "        self.i_x, self.i_h, self.i_b = self._get_params()\n",
    "        self.i2_x, self.i2_h, self.i2_b = self._get_params()\n",
    "        \n",
    "        # Dense Layer for final output\n",
    "#         self.fc = nn.Linear(hidden_dim, 32)\n",
    "#         self.dropout = nn.Dropout(p=0.5)\n",
    "#         self.logits = nn.Linear(32, output_dim)\n",
    "\n",
    "        self.logits = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def _get_params(self):\n",
    "        \n",
    "        x = nn.Parameter(torch.randn([self.input_dim, self.hidden_dim], requires_grad=True, dtype=torch.float32))\n",
    "        h = nn.Parameter(torch.randn([self.hidden_dim, self.hidden_dim], requires_grad=True, dtype=torch.float32))\n",
    "        b = nn.Parameter(torch.randn([1, self.hidden_dim], requires_grad=True, dtype=torch.float32))\n",
    "\n",
    "        return x, h, b\n",
    "    \n",
    "    def _lstm_cell(self, embedded_input, h, state):\n",
    "                \n",
    "        # Forget Gate Calculation\n",
    "        forget_gate = torch.sigmoid(torch.matmul(embedded_input, self.f_x) + torch.matmul(h, self.f_h) + self.f_b)  \n",
    "        \n",
    "        # Output Gate Calculation\n",
    "        output_gate = torch.sigmoid(torch.matmul(embedded_input, self.o_x) + torch.matmul(h, self.o_h) + self.o_b)\n",
    "\n",
    "        # Input Gate Calculation\n",
    "        input_gate = torch.sigmoid(torch.matmul(embedded_input, self.i_x) + torch.matmul(h, self.i_h) + self.i_b) \n",
    "        input2_state = torch.tanh(torch.matmul(embedded_input, self.i2_x) + torch.matmul(h, self.i2_h) + self.i2_b)\n",
    "\n",
    "        # New State after the LSTM Cell\n",
    "        state = input2_state * input_gate + state * forget_gate\n",
    "        \n",
    "        # New Output from the LSTM Cell\n",
    "        h = output_gate * torch.tanh(state)\n",
    "        \n",
    "        return h, state\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x.shape -> [batch_size, 50, 32] \n",
    "\n",
    "        x = self.embedding(torch.from_numpy(x))\n",
    " \n",
    "        h = torch.randn([x.shape[0], hidden_dim], dtype=torch.float32)\n",
    "        state = torch.randn([x.shape[0], hidden_dim], dtype=torch.float32)\n",
    "        \n",
    "        for seq in range(x.shape[1]):\n",
    "            h, state = self._lstm_cell(x[:,seq,:], h, state) \n",
    "\n",
    "        # Flatten the Last Output of the LSTM\n",
    "        flatten_h = h.squeeze()\n",
    "        \n",
    "#         fc1 = torch.relu(self.fc(flatten_h))\n",
    "#         fc2 = self.dropout(fc1)\n",
    "#         logits = self.logits(fc2)\n",
    "\n",
    "        logits = self.logits(flatten_h)\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-ghana",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "minute-smoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "output_dim = 10\n",
    " \n",
    "model = LSTMModel(embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "welcome-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, embeds) in enumerate(test_dataloader):\n",
    "            predited_label = model(embeds)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "distinct-round",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epoch, iter_debug=True):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        iter = 0\n",
    "\n",
    "        for labels, embeds in train_dataloader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(embeds)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            #torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "            optimizer.step()\n",
    "\n",
    "            iter += 1\n",
    "            if iter % 200 == 0 and iter_debug:\n",
    "                print('Epoc {} - Iteration {} - Loss: {}'.format(epoch, iter, loss.item()))   \n",
    "\n",
    "        accuracy = evaluate()\n",
    "        print('Done Training with Epoch {} - Loss: {}. Test Accuracy: {}'.format(epoch, loss.item(), accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fantastic-fitness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Training with Epoch 0 - Loss: 1.4032599925994873. Test Accuracy: 0.2988157894736842\n",
      "Done Training with Epoch 1 - Loss: 1.2664052248001099. Test Accuracy: 0.41736842105263156\n",
      "Done Training with Epoch 2 - Loss: 0.9979702234268188. Test Accuracy: 0.5348684210526315\n",
      "Done Training with Epoch 3 - Loss: 0.826845645904541. Test Accuracy: 0.6551315789473684\n",
      "Done Training with Epoch 4 - Loss: 0.6566196084022522. Test Accuracy: 0.703421052631579\n",
      "Done Training with Epoch 5 - Loss: 0.4938162863254547. Test Accuracy: 0.7343421052631579\n",
      "Done Training with Epoch 6 - Loss: 0.4425247311592102. Test Accuracy: 0.7527631578947368\n",
      "Done Training with Epoch 7 - Loss: 0.37164077162742615. Test Accuracy: 0.7646052631578948\n",
      "Done Training with Epoch 8 - Loss: 0.33144453167915344. Test Accuracy: 0.7735526315789474\n",
      "Done Training with Epoch 9 - Loss: 0.30023258924484253. Test Accuracy: 0.7806578947368421\n"
     ]
    }
   ],
   "source": [
    "train(10, iter_debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
